\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[ant()]{anthropic_claude}
{Claude} with {200K} context length.
\newblock
  \url{https://support.anthropic.com/en/articles/8606395-how-large-is-the-anthropic-api-s-context-window}.
\newblock Accessed: 2024-10-30.

\bibitem[fla()]{flash_decoding}
Flash-decoding for long-context inference.
\newblock \url{https://pytorch.org/blog/flash-decoding/}.
\newblock Accessed: 2024-10-30.

\bibitem[goo()]{google_gemini}
Google's {Gemini} 1.5 {Pro} with {1M} context length.
\newblock
  \url{https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024}.
\newblock Accessed: 2024-10-30.

\bibitem[nvg()]{nvgb200}
Nvidia {GB200} {NVL72}.
\newblock \url{https://www.nvidia.com/en-us/data-center/gb200-nvl72/}.
\newblock Accessed: 2024-10-30.

\bibitem[ope()]{openai_gpt4}
{GPT-4o} with {128K} context length.
\newblock \url{https://platform.openai.com/docs/models}.
\newblock Accessed: 2024-10-30.

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,
  Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.~L.,
  Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Ainslie et~al.(2023)Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy,
  Lebr{\'o}n, and Sanghai]{ainslie2023gqa}
Ainslie, J., Lee-Thorp, J., de~Jong, M., Zemlyanskiy, Y., Lebr{\'o}n, F., and
  Sanghai, S.
\newblock {GQA}: Training generalized multi-query transformer models from
  multi-head checkpoints.
\newblock \emph{arXiv preprint arXiv:2305.13245}, 2023.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
Beltagy, I., Peters, M.~E., and Cohan, A.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem[Brandon et~al.(2023)Brandon, Nrusimha, Qian, Ankner, Jin, Song, and
  Ragan-Kelley]{brandon2023striped}
Brandon, W., Nrusimha, A., Qian, K., Ankner, Z., Jin, T., Song, Z., and
  Ragan-Kelley, J.
\newblock Striped attention: Faster ring attention for causal transformers.
\newblock \emph{arXiv preprint arXiv:2311.09431}, 2023.

\bibitem[Brown(2020)]{brown2020language}
Brown, T.~B.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Cho et~al.(2024)Cho, Rastegari, and Naik]{cho2024kv}
Cho, M., Rastegari, M., and Naik, D.
\newblock Kv-runahead: Scalable causal llm inference by parallel key-value
  cache generation.
\newblock \emph{arXiv preprint arXiv:2405.05329}, 2024.

\bibitem[Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2023palm}
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,
  Barham, P., Chung, H.~W., Sutton, C., Gehrmann, S., et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0
  (240):\penalty0 1--113, 2023.

\bibitem[Dao(2023)]{fa_v2}
Dao, T.
\newblock Flashattention-2: Faster attention with better parallelism and work
  partitioning.
\newblock \emph{arXiv preprint arXiv:2307.08691}, 2023.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and R\'{e}]{fa_v1}
Dao, T., Fu, D., Ermon, S., Rudra, A., and R\'{e}, C.
\newblock Flashattention: Fast and memory-efficient exact attention with
  io-awareness.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and
  Oh, A. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~35, pp.\  16344--16359. Curran Associates, Inc., 2022.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf}.

\bibitem[Devlin(2018)]{devlin2018bert}
Devlin, J.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Fang \& Zhao(2024)Fang and Zhao]{fang2024usp}
Fang, J. and Zhao, S.
\newblock Usp: A unified sequence parallelism approach for long context
  generative ai.
\newblock \emph{arXiv preprint arXiv:2405.07719}, 2024.

\bibitem[{Gemini Team}(2023)]{team2023gemini}
{Gemini Team}.
\newblock {Gemini}: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[{Gemini Team}(2024)]{reid2024gemini}
{Gemini Team}.
\newblock {Gemini 1.5}: Unlocking multimodal understanding across millions of
  tokens of context.
\newblock \emph{arXiv preprint arXiv:2403.05530}, 2024.

\bibitem[Hooper et~al.(2024)Hooper, Kim, Mohammadzadeh, Mahoney, Shao, Keutzer,
  and Gholami]{hooper2024kvquant}
Hooper, C., Kim, S., Mohammadzadeh, H., Mahoney, M.~W., Shao, Y.~S., Keutzer,
  K., and Gholami, A.
\newblock Kvquant: Towards 10 million context length llm inference with kv
  cache quantization.
\newblock \emph{arXiv preprint arXiv:2401.18079}, 2024.

\bibitem[Huang et~al.(2019)Huang, Cheng, Bapna, Firat, Chen, Chen, Lee, Ngiam,
  Le, Wu, et~al.]{huang2019gpipe}
Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam,
  J., Le, Q.~V., Wu, Y., et~al.
\newblock Gpipe: Efficient training of giant neural networks using pipeline
  parallelism.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Jacobs et~al.(2023)Jacobs, Tanaka, Zhang, Zhang, Song, Rajbhandari,
  and He]{jacobs2023deepspeed}
Jacobs, S.~A., Tanaka, M., Zhang, C., Zhang, M., Song, S.~L., Rajbhandari, S.,
  and He, Y.
\newblock Deepspeed ulysses: System optimizations for enabling training of
  extreme long sequence transformer models.
\newblock \emph{arXiv preprint arXiv:2309.14509}, 2023.

\bibitem[Jiang et~al.(2024)Jiang, Li, Zhang, Wu, Luo, Ahn, Han, Abdi, Li, Lin,
  et~al.]{jiang2024minference}
Jiang, H., Li, Y., Zhang, C., Wu, Q., Luo, X., Ahn, S., Han, Z., Abdi, A.~H.,
  Li, D., Lin, C.-Y., et~al.
\newblock Minference 1.0: Accelerating pre-filling for long-context llms via
  dynamic sparse attention.
\newblock \emph{arXiv preprint arXiv:2407.02490}, 2024.

\bibitem[Juravsky et~al.(2024)Juravsky, Brown, Ehrlich, Fu, R{\'e}, and
  Mirhoseini]{juravsky2024hydragen}
Juravsky, J., Brown, B., Ehrlich, R., Fu, D.~Y., R{\'e}, C., and Mirhoseini, A.
\newblock Hydragen: High-throughput llm inference with shared prefixes.
\newblock \emph{arXiv preprint arXiv:2402.05099}, 2024.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R.,
  Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Korthikanti et~al.(2023)Korthikanti, Casper, Lym, McAfee, Andersch,
  Shoeybi, and Catanzaro]{korthikanti2023reducing}
Korthikanti, V.~A., Casper, J., Lym, S., McAfee, L., Andersch, M., Shoeybi, M.,
  and Catanzaro, B.
\newblock Reducing activation recomputation in large transformer models.
\newblock \emph{Proceedings of Machine Learning and Systems}, 5:\penalty0
  341--353, 2023.

\bibitem[Kwon et~al.(2023)Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang,
  and Stoica]{kwon2023efficient}
Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C.~H., Gonzalez, J.,
  Zhang, H., and Stoica, I.
\newblock Efficient memory management for large language model serving with
  pagedattention.
\newblock In \emph{Proceedings of the 29th Symposium on Operating Systems
  Principles}, pp.\  611--626, 2023.

\bibitem[Li et~al.(2023)Li, Shao, Xie, Xing, Ma, Stoica, Gonzalez, and
  Zhang]{li2023distflashattn}
Li, D., Shao, R., Xie, A., Xing, E.~P., Ma, X., Stoica, I., Gonzalez, J.~E.,
  and Zhang, H.
\newblock Distflashattn: Distributed memory-efficient attention for
  long-context llms training.
\newblock \emph{arXiv preprint arXiv:2310.03294}, 2023.

\bibitem[Li et~al.(2021)Li, Xue, Baranwal, Li, and You]{li2021sequence}
Li, S., Xue, F., Baranwal, C., Li, Y., and You, Y.
\newblock Sequence parallelism: Long sequence training from system perspective.
\newblock \emph{arXiv preprint arXiv:2105.13120}, 2021.

\bibitem[Lin et~al.(2024)Lin, Tang, Yang, Zhang, Xiao, Gan, and
  Han]{lin2024qserve}
Lin, Y., Tang, H., Yang, S., Zhang, Z., Xiao, G., Gan, C., and Han, S.
\newblock Qserve: W4a8kv4 quantization and system co-design for efficient llm
  serving.
\newblock \emph{arXiv preprint arXiv:2405.04532}, 2024.

\bibitem[Liu et~al.(2023)Liu, Zaharia, and Abbeel]{liu2023ring}
Liu, H., Zaharia, M., and Abbeel, P.
\newblock Ring attention with blockwise transformers for near-infinite context.
\newblock \emph{arXiv preprint arXiv:2310.01889}, 2023.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{liu2021swin}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pp.\  10012--10022, 2021.

\bibitem[{Llama Team}(2024)]{llama3}
{Llama Team}.
\newblock The {Llama} 3 herd of models.
\newblock 2024.
\newblock URL \url{https://arxiv.org/abs/2407.21783}.

\bibitem[{Meta Engineering}(2022)]{gt2022}
{Meta Engineering}.
\newblock {OCP} summit 2022: Open hardware for ai infrastructure.
\newblock
  \url{https://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/},
  2022.
\newblock Accessed: 2024-10-30.

\bibitem[Milakov \& Gimelshein(2018)Milakov and Gimelshein]{milakov2018online}
Milakov, M. and Gimelshein, N.
\newblock Online normalizer calculation for softmax.
\newblock \emph{arXiv preprint arXiv:1805.02867}, 2018.

\bibitem[Munkhdalai et~al.(2024)Munkhdalai, Faruqui, and
  Gopal]{munkhdalai2024leave}
Munkhdalai, T., Faruqui, M., and Gopal, S.
\newblock Leave no context behind: Efficient infinite context transformers with
  infini-attention.
\newblock \emph{arXiv preprint arXiv:2404.07143}, 2024.

\bibitem[Narayanan et~al.(2021)Narayanan, Shoeybi, Casper, LeGresley, Patwary,
  Korthikanti, Vainbrand, Kashinkunti, Bernauer, Catanzaro,
  et~al.]{narayanan2021efficient}
Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M.,
  Korthikanti, V., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B.,
  et~al.
\newblock Efficient large-scale language model training on gpu clusters using
  megatron-lm.
\newblock In \emph{Proceedings of the International Conference for High
  Performance Computing, Networking, Storage and Analysis}, pp.\  1--15, 2021.

\bibitem[{Nvidia Blog}(2019)]{cudagraph}
{Nvidia Blog}.
\newblock Getting started with {CUDA} {Graphs}.
\newblock \url{https://developer.nvidia.com/blog/cuda-graphs/}, 2019.
\newblock Accessed: 2024-10-30.

\bibitem[Pope et~al.(2023)Pope, Douglas, Chowdhery, Devlin, Bradbury, Heek,
  Xiao, Agrawal, and Dean]{pope2023efficiently}
Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J., Heek, J., Xiao,
  K., Agrawal, S., and Dean, J.
\newblock Efficiently scaling transformer inference.
\newblock \emph{Proceedings of Machine Learning and Systems}, 5:\penalty0
  606--624, 2023.

\bibitem[Qin et~al.(2024)Qin, Li, He, Zhang, Wu, Zheng, and
  Xu]{qin2407mooncake}
Qin, R., Li, Z., He, W., Zhang, M., Wu, Y., Zheng, W., and Xu, X.
\newblock Mooncake: A kvcache-centric disaggregated architecture for llm
  serving.
\newblock \emph{URL https://arxiv. org/abs/2407.00079}, 2024.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Shah et~al.(2024)Shah, Bikshandi, Zhang, Thakkar, Ramani, and
  Dao]{shah2024flashattention}
Shah, J., Bikshandi, G., Zhang, Y., Thakkar, V., Ramani, P., and Dao, T.
\newblock Flashattention-3: Fast and accurate attention with asynchrony and
  low-precision.
\newblock \emph{arXiv preprint arXiv:2407.08608}, 2024.

\bibitem[Shazeer(2019)]{shazeer2019fast}
Shazeer, N.
\newblock Fast transformer decoding: One write-head is all you need.
\newblock \emph{arXiv preprint arXiv:1911.02150}, 2019.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{shoeybi2019megatron}
Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro,
  B.
\newblock Megatron-lm: Training multi-billion parameter language models using
  model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,
  Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{llama1}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
  T., Rozi{\`e}re, B., Goyal, N., Hambro, E., Azhar, F., et~al.
\newblock {Llama}: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert,
  Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{llama2}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y.,
  Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[Vaswani(2017)]{vaswani2017attention}
Vaswani, A.
\newblock Attention is all you need.
\newblock \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
Wang, S., Li, B.~Z., Khabsa, M., Fang, H., and Ma, H.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[Wu et~al.(2024)Wu, Liu, Zhong, Sun, Liu, and Jin]{wu2024loongserve}
Wu, B., Liu, S., Zhong, Y., Sun, P., Liu, X., and Jin, X.
\newblock Loongserve: Efficiently serving long-context large language models
  with elastic sequence parallelism.
\newblock In \emph{Proceedings of the ACM SIGOPS 30th Symposium on Operating
  Systems Principles}, pp.\  640--654, 2024.

\bibitem[Xiao et~al.(2023)Xiao, Tian, Chen, Han, and Lewis]{xiao2023efficient}
Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M.
\newblock Efficient streaming language models with attention sinks.
\newblock \emph{arXiv preprint arXiv:2309.17453}, 2023.

\bibitem[Xiong et~al.(2021)Xiong, O{\u{g}}uz, Gupta, Chen, Liskovich, Levy,
  Yih, and Mehdad]{xiong2021simple}
Xiong, W., O{\u{g}}uz, B., Gupta, A., Chen, X., Liskovich, D., Levy, O., Yih,
  W.-t., and Mehdad, Y.
\newblock Simple local attentions remain competitive for long-context tasks.
\newblock \emph{arXiv preprint arXiv:2112.07210}, 2021.

\bibitem[Zhong et~al.(2024)Zhong, Liu, Chen, Hu, Zhu, Liu, Jin, and
  Zhang]{zhong2024distserve}
Zhong, Y., Liu, S., Chen, J., Hu, J., Zhu, Y., Liu, X., Jin, X., and Zhang, H.
\newblock Distserve: Disaggregating prefill and decoding for goodput-optimized
  large language model serving, 2024.

\end{thebibliography}
